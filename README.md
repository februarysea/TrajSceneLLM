# TrajSceneLLM: Trajectory Scene Understanding with the GeoLife Dataset

This project provides a complete, configurable pipeline for processing, analyzing, and classifying GPS trajectories from the **Microsoft GeoLife 1.3 dataset**. It leverages Large Language Models (LLMs) and multimodal embedding techniques to understand and categorize trajectories based on their spatial patterns, textual descriptions, and visual representations.

The core idea is to represent trajectories not just as a series of points, but as rich scenes combining **visualizations** (map images) and **textual descriptions** (generated by Large Language Models). These multimodal representations are then used to train classifiers for tasks like transportation mode detection.

## Key Features

- **Designed for GeoLife**: A complete, end-to-end workflow tailored for the popular GeoLife 1.3 dataset.
- **Multimodal Feature Generation**:
    - **Textual Descriptions**: Generates rich, human-like descriptions of trajectories using LLMs (e.g., DeepSeek).
    - **Image Visualizations**: Creates static map images for each trajectory.
- **Two Multimodal Strategies**:
    - **Early Fusion (`multimodal`)**: A sophisticated approach where text and images are fused into a single, semantically rich embedding by a specialized multimodal LLM.
    - **Late Fusion (`image text`)**: A powerful baseline where separate text and image embeddings are generated and then concatenated to form a combined feature vector.
- **Modular and Configurable**: The entire workflow—from file paths to model hyperparameters—is controlled via a central `config.yaml` file.

---

## How It Works: Early vs. Late Fusion

This framework implements two distinct strategies for combining text and image data:

1.  **Early Fusion (Fused Multimodal Embeddings)**
    -   **How**: Image data and text descriptions are sent *together* in a single request to a multimodal model (e.g., VolcEngine's `multimodal-embedding` endpoint).
    -   **Result**: A single, unified embedding vector that captures the deep semantic relationship between the image and the text.
    -   **Usage**: `--embedding_mode multimodal`

2.  **Late Fusion (Concatenated Embeddings)**
    -   **How**: Image and text embeddings are generated *separately* in two different steps. During training, the framework loads both sets of embeddings and simply concatenates them end-to-end.
    -   **Result**: A longer feature vector that combines the independent strengths of both modalities.
    -   **Usage**: `--embedding_types image text`

---

## Setup and Configuration

### 1. Installation
Clone the repository, create a Python virtual environment, and install the dependencies.
```bash
git clone https://github.com/februarysea/TrajSceneLLM
cd TrajSceneLLM
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. API Keys
Create a `.env` file in the project root to store your API keys. The project will not run without this.
```
# .env file
VOLC_API_KEY="your_volc_engine_api_key"   # For image and multimodal embeddings
DEEPSEEK_API_KEY="your_deepseek_api_key" # For text description generation
```

### 3. Download the GeoLife Dataset
1.  Download the **[GeoLife GPS Trajectories 1.3](https://www.microsoft.com/en-us/download/details.aspx?id=52367)** dataset from the official Microsoft website.
2.  Extract the archive.
3.  Place the extracted `Geolife Trajectories 1.3` folder (which contains the `Data` directory) into the `data/raw/` directory of this project. The final path should look like: `data/raw/Geolife Trajectories 1.3/Data/...`.

### 4. Project Configuration (`config.yaml`)
The configuration file at `config/config.yaml` is pre-set for the GeoLife dataset. You can review it to adjust parameters if needed.

-   **`paths`**: Paths are configured for the default GeoLife structure. No changes are needed if you followed the setup above.
-   **`project`**: The `map_center` is set to Beijing, the location of the GeoLife data.
-   **`llm_prompts`**: You can customize the prompts used for generating textual descriptions.
-   **`training`**: Adjust hyperparameters for the MLP model, such as `epochs`, `batch_size`, and `learning_rate`.

---

## Running the Pipeline

The pipeline is controlled via `main.py`. You can run individual steps or chain them together.

### Core Steps
- `clean_data`: Cleans the raw GeoLife trajectory files.
- `generate_sequences`: Groups points into sequences (required by all subsequent steps).
- `generate_images`: Creates map images for each trajectory.
- `generate_text`: Generates textual descriptions using the configured LLM.
- `generate_embeddings`: Creates vector embeddings. Use with `--embedding_mode`.
- `train_embedding_model`: Trains the classifier. Use with `--embedding_types`.

### Example Workflows

**1. Full Pipeline with Late Fusion (Concatenation)**
This is a robust and recommended workflow.
```bash
# Step 1: Clean raw data and generate sequences
python3 main.py --steps clean_data generate_sequences

# Step 2: Generate features and embeddings
python3 main.py --steps generate_text generate_images
python3 main.py --steps generate_embeddings --embedding_mode image
python3 main.py --steps generate_embeddings --embedding_mode text

# Step 3: Train on concatenated embeddings
python3 main.py --steps train_embedding_model --embedding_types image text
```

**2. Full Pipeline with Early Fusion**
```bash
# Step 1: Clean raw data and generate sequences, text, and images
python3 main.py --steps clean_data generate_sequences generate_text generate_images

# Step 2: Generate the fused multimodal embeddings
python3 main.py --steps generate_embeddings --embedding_mode multimodal

# Step 3: Train on the fused embeddings
python3 main.py --steps train_embedding_model --embedding_types multimodal
```

---

## Outputs

After running the training pipeline, the following outputs will be generated in a timestamped subdirectory inside `models/embedding_models/`:

-   **`best_model.pth`**: The saved weights of the best-performing model based on validation accuracy.
-   **`classification_report.csv`**: A detailed report with precision, recall, and F1-score for each class.
-   **`confusion_matrix.png`**: A visualization of the model's predictions on the test set.
-   **`training_log.log`**: A complete log of the training process.
-   **`training_plots.png`**: Plots showing the training & validation loss and accuracy over epochs. 